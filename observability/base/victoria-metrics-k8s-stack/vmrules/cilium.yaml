apiVersion: operator.victoriametrics.com/v1beta1
kind: VMRule
metadata:
  name: cilium
  namespace: kube-system
spec:
  groups:
    - name: cilium-agent
      rules:
        - alert: CiliumAgentUnreachableHealthEndpoints
          expr: |
            max by (namespace, pod) (cilium_unreachable_health_endpoints) > 0
            and on()
            (
              (
                count(changes(kube_pod_info{created_by_name="agent",namespace="kube-system"}[2m])) -
                count(kube_pod_info{created_by_name="agent",namespace="kube-system"} offset 2m)
              ) == 0
            )
          for: 5m
          labels:
            severity: critical
          annotations:
            message: Some node's health endpoints are not reachable by agent {{ $labels.namespace }}/{{ $labels.pod }}.
            description: |
              Check what's going on: `kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}`
            runbook_url: "https://docs.cilium.io/en/stable/operations/troubleshooting/"
            dashboard: "https://grafana.priv.${domain_name}/dashboards"

        - alert: CiliumAgentMetricNotFound
          expr: (count by (namespace,pod) (cilium_unreachable_health_endpoints) OR count by (namespace,pod) (cilium_endpoint_state)) != 1
          for: 5m
          labels:
            severity: critical
          annotations:
            message: Some of the metrics are not coming from the agent {{ $labels.namespace }}/{{ $labels.pod }}.
            description: |
              Use the following commands to check what's going on:
              - `kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}`
              - `kubectl -n {{ $labels.namespace }} exec -ti {{ $labels.pod }} cilium-health status`
              We need to cross-check the metrics with the neighboring agent.
              Also, the absence of metrics is an indirect sign that new pods cannot be created on the node because of the inability to connect to the agent.
              It is important to get a more specific way of determining the above situation and create a more accurate alert for the inability to connect new pods to the agent.
            runbook_url: "https://docs.cilium.io/en/stable/operations/troubleshooting/"
            dashboard: "https://grafana.priv.${domain_name}/dashboards"

        - alert: CiliumAgentEndpointsNotReady
          expr: sum by (namespace, pod) (cilium_endpoint_state{endpoint_state="ready"} / cilium_endpoint_state) < 0.5
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: More than half of all known Endpoints are not ready in agent {{ $labels.namespace }}/{{ $labels.pod }}.
            description: |
              Check what's going on: `kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}`
            runbook_url: "https://docs.cilium.io/en/stable/operations/troubleshooting/"
            dashboard: "https://grafana.priv.${domain_name}/dashboards"

        - alert: CiliumAgentPolicyImportErrors
          expr: sum by (namespace, pod) (rate(cilium_policy_import_errors_total[2m]) > 0)
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Agent {{ $labels.namespace }}/{{ $labels.pod }} fails to import policies.
            description: |
              Check what's going on: `kubectl -n {{ $labels.namespace }} logs {{ $labels.pod }}`
            runbook_url: "https://docs.cilium.io/en/stable/operations/troubleshooting/"
            dashboard: "https://grafana.priv.${domain_name}/dashboards"
